{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Latamila/PyTorch-e-Lightning/blob/main/LMM_Modelagem_Deploy_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6F4rST8ZqpT"
      },
      "source": [
        "# <font color='blue'>Data Science Academy</font>\n",
        "# <font color='blue'>Deep Learning Para Aplicações de IA com PyTorch e Lightning</font>\n",
        "\n",
        "## <font color='blue'>Mini-Projeto 3 - Modelagem</font>\n",
        "## <font color='blue'>Fine-Tuning de Modelo LLM Para Tarefa Específica e Deploy de Web App com Gradio</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEgqGuapZqpX"
      },
      "source": [
        "![DSA](imagens/MP3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ8RAIXVZqpY"
      },
      "source": [
        "## Instalando e Carregando os Pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3JNxDhrZqpY",
        "outputId": "cbb37cde-a19d-4fea-d9f4-1b4f7764e4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.10.12\n"
          ]
        }
      ],
      "source": [
        "# Versão da Linguagem Python\n",
        "from platform import python_version\n",
        "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESqJ-7W5ZqpZ",
        "outputId": "7aecc6b9-2af7-4895-8b2a-370f21be0d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.4/1.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# pip install -U nome_pacote\n",
        "\n",
        "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# !pip install nome_pacote==versão_desejada\n",
        "\n",
        "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
        "\n",
        "# Instala o pacote watermark.\n",
        "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9piFU0MpZqpa",
        "outputId": "c14c51a5-db0b-4bfa-b47e-f306d5d91a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TF_CPP_MIN_LOG_LEVEL=3\n"
          ]
        }
      ],
      "source": [
        "%env TF_CPP_MIN_LOG_LEVEL=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp46MbF0Zqpa",
        "outputId": "cfd67780-24ed-4717-d834-d5cf74f3aeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZk0VVStZqpb",
        "outputId": "342e6588-ca38-415c-d301-d3b15905a95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.31.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-bDPrFwZqpb",
        "outputId": "7950b324-ebd8-4df5-db2a-f97e5a3ff564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/280.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m256.0/280.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pgO9fKLTZqpb"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from accelerate import Accelerator\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, GPT2Tokenizer\n",
        "from transformers import GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J46lqZYiZqpb",
        "outputId": "5cb02f52-7d11-4964-9c07-73ac5bf4f588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Data Science Academy\n",
            "\n",
            "torch       : 2.0.1\n",
            "numpy       : 1.25.2\n",
            "transformers: 4.31.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Data Science Academy\" --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLbxNMh0Zqpc"
      },
      "source": [
        "## Carregando o LLM\n",
        "\n",
        "https://huggingface.co/gpt2\n",
        "\n",
        "O modelo terá a mesma arquitetura do GPT-2, mas com algumas modificações para torná-lo menor. As principais mudanças são o tamanho do vocabulário que é 13 porque só vai lidar com números mais o padding token, o \"+\" e o \"=\". A janela de contexto suportará apenas 6 tokens, pois estamos interessados apenas em realizar a adição de dois dígitos únicos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LyVgPjGWZqpc"
      },
      "outputs": [],
      "source": [
        "# Tamanho do vocabulário\n",
        "vocab_size = 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1XFm1DxqZqpd"
      },
      "outputs": [],
      "source": [
        "# Comprimento da sequência\n",
        "sequence_length = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NM06KekJZqpd"
      },
      "outputs": [],
      "source": [
        "# Comprimento do resultado\n",
        "result_length = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C3O6HwJ6Zqpd"
      },
      "outputs": [],
      "source": [
        "# Comprimento do contexto\n",
        "context_length = sequence_length + result_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "4f0f32650ba44908802859efc387f7d8",
            "8bbed4a54c8348cf973bb77aade30405",
            "e5141ea4c4184a76b9d08c7100e96896",
            "dff7cc11b12c4cf9bdb27a40efa932ac",
            "f0ffd0c2cfdf465c905209babf2110fa",
            "ef77cfdab1d34873894bd06fccf7ae9c",
            "b9864979ec2f41c88c46becf3055a910",
            "458ce2d4765c47bdb93a7611acecd673",
            "8f624370d7a34bb5b2ac456438daebcb",
            "cbb34f81b9f148978a22d8359e17f8fd",
            "9abe9c5798434432949d45497beb3995"
          ]
        },
        "id": "6BHyJXijZqpe",
        "outputId": "6ab49e4f-d1f2-48ad-8adc-acd7f51cff4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f0f32650ba44908802859efc387f7d8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Parâmetros de configuração do modelo GPT-2\n",
        "config = AutoConfig.from_pretrained(\"gpt2\",\n",
        "                                    vocab_size = vocab_size,\n",
        "                                    n_ctx = context_length,\n",
        "                                    n_head = 4,\n",
        "                                    n_layer = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M3wGtFlcZqpe"
      },
      "outputs": [],
      "source": [
        "# Carrega o modelo\n",
        "modelo = AutoModelForCausalLM.from_config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "F5v8ZnuSZqpe"
      },
      "outputs": [],
      "source": [
        "# Função para calcular o tamanho do modelo\n",
        "def model_size(model):\n",
        "    return sum(t.numel() for t in modelo.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS3IodiLZqpe",
        "outputId": "e1c4b965-4ef5-45f9-c548-19780c9e382b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do Modelo: 15.0M parâmetros\n"
          ]
        }
      ],
      "source": [
        "print(f'Tamanho do Modelo: {model_size(modelo)/1000**2:.1f}M parâmetros')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFP3M0BSZqpf"
      },
      "source": [
        "Este modelo tem 15 milhões de parâmetros em vez dos 111 milhões de parâmetros da configuração padrão \"gpt2\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "4QqHtLNZZqpf",
        "outputId": "fa826c74-c70a-4dd3-92fe-0ae3b42f1704"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel</b><br/>def _call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py</a>The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input\n",
              "embeddings).\n",
              "\n",
              "\n",
              "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
              "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
              "etc.)\n",
              "\n",
              "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
              "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
              "and behavior.\n",
              "\n",
              "Parameters:\n",
              "    config ([`GPT2Config`]): Model configuration class with all the parameters of the model.\n",
              "        Initializing with a config file does not load the weights associated with the model, only the\n",
              "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 949);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "type(modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mHETOv-5Zqpf"
      },
      "outputs": [],
      "source": [
        "# Salva o modelo em disco\n",
        "modelo.save_pretrained(\"modelos/modelo_inicial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCmVz0VjZqpf"
      },
      "source": [
        "## Tokenizador Personalizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XBYoncv1Zqpf"
      },
      "outputs": [],
      "source": [
        "# Definindo uma classe chamada NumberTokenizer, que é usada para tokenizar os números\n",
        "class DSATokenizer:\n",
        "\n",
        "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
        "    def __init__(self, numbers_qty = 10):\n",
        "\n",
        "        # Lista de tokens possíveis que o tokenizador pode encontrar\n",
        "        vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "        # Definindo a quantidade de números que o tokenizador pode lidar\n",
        "        self.numbers_qty = numbers_qty\n",
        "\n",
        "        # Definindo o token de preenchimento (padding)\n",
        "        self.pad_token = '-1'\n",
        "\n",
        "        # Criando um dicionário que mapeia cada token para um índice único\n",
        "        self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
        "\n",
        "        # Criando um dicionário que mapeia cada índice único de volta ao token correspondente\n",
        "        self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
        "\n",
        "        # Obtendo o índice do token de preenchimento no encoder\n",
        "        self.pad_token_id = self.encoder[self.pad_token]\n",
        "\n",
        "    # Método para decodificar uma lista de IDs de token de volta para uma string\n",
        "    def decode(self, token_ids):\n",
        "        return ' '.join(self.decoder[t] for t in token_ids)\n",
        "\n",
        "    # Método que é chamado quando o objeto da classe é invocado como uma função\n",
        "    def __call__(self, text):\n",
        "        # Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\n",
        "        return [self.encoder[t] for t in text.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vvoC1A5Zqpg"
      },
      "source": [
        "> Vamos testar o tokenizador!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2Hb6vAByZqpg"
      },
      "outputs": [],
      "source": [
        "# Cria o objeto do tokenizador\n",
        "tokenizer = DSATokenizer(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMhPzjJHZqpg",
        "outputId": "41cde013-e9a5-4af7-a49b-58d8e2e4ebef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '+',\n",
              " 1: '=',\n",
              " 2: '-1',\n",
              " 3: '0',\n",
              " 4: '1',\n",
              " 5: '2',\n",
              " 6: '3',\n",
              " 7: '4',\n",
              " 8: '5',\n",
              " 9: '6',\n",
              " 10: '7',\n",
              " 11: '8',\n",
              " 12: '9'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Decoder do tokenizador\n",
        "tokenizer.decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzCsjyz2Zqph",
        "outputId": "40ca6f0e-4a27-4cf1-e59c-ed12d55c5e0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 0, 4, 1, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Testando o tokenizador\n",
        "tokenizer(\"1 + 1 = 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ooMCsTaZqph"
      },
      "source": [
        "## Criando o Dataset\n",
        "\n",
        "O conjunto de dados deve ser criado neste formato:\n",
        "\n",
        "- Entrada: \"2 + 3 = 0\" onde os 4 primeiros caracteres representam a sequência de entrada e o quinto caractere representa o primeiro caracter da saída.\n",
        "\n",
        "- Saída: \"+ 3 = 0 5\" onde os 2 últimos dígitos representam o resultado da adição e os 3 primeiros dígitos são ignorados durante o treinamento e preenchidos com o pad.\n",
        "\n",
        "O resultado é um conjunto de dados de sequências tokenizadas de números."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b8DLpqa3Zqpi"
      },
      "outputs": [],
      "source": [
        "# Definindo uma classe chamada CriaDataset, que herda da classe Dataset do PyTorch\n",
        "class CriaDataset(Dataset):\n",
        "\n",
        "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
        "    def __init__(self, split, length = 6):\n",
        "\n",
        "        # Verificando se a divisão do dataset (split) é 'treino' ou 'teste'\n",
        "        assert split in {'treino', 'teste'}\n",
        "        self.split = split\n",
        "        self.length = length\n",
        "\n",
        "    # Definindo o método len que retorna o tamanho do dataset.\n",
        "    # Nesse caso, o tamanho é fixo e igual a 1 milhão.\n",
        "    def __len__(self):\n",
        "        return 1000000\n",
        "\n",
        "    # Definindo o método getitem que é usado para obter um item específico do dataset\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Criando uma lista com todos os números disponíveis que não são tokens de padding e são numéricos\n",
        "        available_numbers = [int(n) for n in tokenizer.decoder.values() if n != tokenizer.pad_token and str(n).isnumeric()]\n",
        "\n",
        "        # Selecionando aleatoriamente números da lista de números disponíveis para criar uma entrada (input)\n",
        "        inp = torch.tensor(np.random.choice(available_numbers, size = result_length))\n",
        "\n",
        "        # Calculando a soma dos números selecionados e criando um tensor\n",
        "        sol = torch.tensor([int(i) for i in str(inp.sum().item())])\n",
        "\n",
        "        # Preenchendo o tensor com zeros para que tenha o tamanho desejado\n",
        "        sol = torch.nn.functional.pad(sol, (1 if sol.size()[0] == 1 else 0,0), 'constant', 0)\n",
        "\n",
        "        # Concatenando a entrada e a solução em um tensor\n",
        "        cat = torch.cat((inp, sol), dim = 0)\n",
        "\n",
        "        # Criando os tensores de entrada e alvo para o treinamento do modelo\n",
        "        x = cat[:-1].clone()\n",
        "        y = cat[1:].clone()\n",
        "\n",
        "        # Definindo o primeiro elemento do tensor alvo como o token de padding\n",
        "        y[:1] = int(tokenizer.pad_token)\n",
        "\n",
        "        # Transformando os tensores x e y em strings\n",
        "        x = str(x[0].item()) + ' + ' + str(x[1].item()) + ' = ' + str(x[2].item())\n",
        "        y = '-1 ' + str(y[0].item()) + ' -1 ' + str(y[1].item()) + ' ' + str(y[2].item())\n",
        "\n",
        "        # Tokenizando as strings de entrada e alvo\n",
        "        tokenized_input = tokenizer(x)\n",
        "        tokenized_output = tokenizer(y)\n",
        "\n",
        "        # Retornando os tensores de entrada e alvo como itens do dataset\n",
        "        return torch.tensor(tokenized_input), torch.tensor(tokenized_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmYGESrPZqpi"
      },
      "source": [
        "## Datasets de Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "auNOevogZqpj"
      },
      "outputs": [],
      "source": [
        "dataset_treino = CriaDataset('treino', length = sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fKP7jtrlZqpj"
      },
      "outputs": [],
      "source": [
        "dataset_teste = CriaDataset('teste', length = sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CRAYtygeZqpj"
      },
      "outputs": [],
      "source": [
        "x, y = dataset_treino[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBWjqpTUZqpk",
        "outputId": "6d2bcf90-ac23-40df-fe98-325c463702b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12,  0, 11,  1,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH5FtNWfZqpk",
        "outputId": "2bd1ae0f-f00e-4af8-b9ee-84dbeea9188e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2,  2,  2,  4, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFkZc1pYZqpk",
        "outputId": "5bc69f3e-3a93-4fc8-ba47-e0a2c45afd31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 + 8 = 1\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(x.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FuVFvtMZqpl",
        "outputId": "19c7148b-9ae2-479c-da42-9dd032b6052b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 -1 -1 1 7\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(y.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycy1v7giZqpl"
      },
      "source": [
        "## Loop de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SC0M4E8nZqpm"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DpyOn2X6Zqpm"
      },
      "outputs": [],
      "source": [
        "batch_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xC0_xPMSZqpm"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(modelo.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oMjZnZCWZqpm"
      },
      "outputs": [],
      "source": [
        "dados = torch.utils.data.DataLoader(dataset_treino, shuffle = True, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sJc8HPVZqpn"
      },
      "source": [
        "https://pypi.org/project/accelerate/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6TyC_f-XZqpn"
      },
      "outputs": [],
      "source": [
        "import accelerate\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wPLoP_gOZqpn"
      },
      "outputs": [],
      "source": [
        "accelerator = Accelerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIJTvwoMZqpn",
        "outputId": "5a0c64be-2779-49ac-b7e1-1a0a83026720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Data Science Academy\n",
            "\n",
            "numpy       : 1.25.2\n",
            "transformers: 4.31.0\n",
            "accelerate  : 0.27.2\n",
            "torch       : 2.0.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Data Science Academy\" --iversions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AGYNXcdXZqpo"
      },
      "outputs": [],
      "source": [
        "modelo, optimizer, dados = accelerator.prepare(modelo, optimizer, dados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig8tb_OBZqpo",
        "outputId": "896098e1-a235-4936-b882-574c1dc64327"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(13, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-1): 2 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=13, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "modelo.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtvw2zxmZqpo",
        "outputId": "021ba5d6-2687-4627-f51c-114ab22d92e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2 --- Erro: 6.514172855531797e-05\n",
            "Epoch: 2/2 --- Erro: 1.7583369071871857e-07\n"
          ]
        }
      ],
      "source": [
        "# Iniciando o loop para as épocas de treinamento\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Iterando por cada batch (conjunto) de dados de entrada e alvos no dataset de treinamento\n",
        "    for source, targets in dados:\n",
        "\n",
        "        # Resetando os gradientes acumulados no otimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculando a perda (loss) através da entropia cruzada entre as previsões do modelo e os alvos verdadeiros.\n",
        "        # Os tensores são \"achatados\" para que possam ser passados para a função de entropia cruzada.\n",
        "        # O índice do token de preenchimento (pad_token) é ignorado no cálculo da perda.\n",
        "        loss = F.cross_entropy(modelo(source).logits.flatten(end_dim = 1),\n",
        "                               targets.flatten(end_dim = 1),\n",
        "                               ignore_index = tokenizer.pad_token_id)\n",
        "\n",
        "        # Calculando os gradientes da perda em relação aos parâmetros do modelo\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        # Atualizando os parâmetros do modelo utilizando os gradientes calculados\n",
        "        optimizer.step()\n",
        "\n",
        "        # Recalculando a perda após a etapa de otimização.\n",
        "        loss = F.cross_entropy(modelo(source).logits.flatten(end_dim = 1),\n",
        "                               targets.flatten(end_dim = 1),\n",
        "                               ignore_index = tokenizer.pad_token_id)\n",
        "\n",
        "    # Imprimindo a época atual e a perda após cada época de treinamento\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs} --- Erro: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EmucKAZqpp"
      },
      "source": [
        "## Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QpnDJiraZqpp"
      },
      "outputs": [],
      "source": [
        "# Definindo a função gera_solution com três parâmetros: input, solution_length e model\n",
        "def faz_previsao(entrada, solution_length = 6, model = modelo):\n",
        "\n",
        "    # Colocando o modelo em modo de avaliação.\n",
        "    model.eval()\n",
        "\n",
        "    # Convertendo a entrada (string) em tensor utilizando o tokenizer.\n",
        "    # O tensor é uma estrutura de dados que o modelo de aprendizado de máquina pode processar.\n",
        "    entrada = torch.tensor(tokenizer(entrada))\n",
        "\n",
        "    # Enviando o tensor de entrada para o dispositivo de cálculo disponível (CPU ou GPU)\n",
        "    entrada = entrada.to(accelerator.device)\n",
        "\n",
        "    # Iniciando uma lista vazia para armazenar a solução\n",
        "    solution = []\n",
        "\n",
        "    # Loop que gera a solução de comprimento solution_length\n",
        "    for i in range(solution_length):\n",
        "\n",
        "        # Alimentando a entrada atual ao modelo e obtendo a saída\n",
        "        saida = model(entrada)\n",
        "\n",
        "        # Pegando o índice do maior valor no último conjunto de logits (log-odds) da saída,\n",
        "        # que é a previsão do modelo para o próximo token\n",
        "        predicted = saida.logits[-1].argmax()\n",
        "\n",
        "        # Concatenando a previsão atual com a entrada atual.\n",
        "        # Isso servirá como a nova entrada para a próxima iteração.\n",
        "        entrada = torch.cat((entrada, predicted.unsqueeze(0)), dim = 0)\n",
        "\n",
        "        # Adicionando a previsão atual à lista de soluções e convertendo o tensor em um número Python padrão\n",
        "        solution.append(predicted.cpu().item())\n",
        "\n",
        "    # Decodificando a lista de soluções para obter a string de saída e retornando-a\n",
        "    return tokenizer.decode(solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "c25wdEm9Zqpp"
      },
      "outputs": [],
      "source": [
        "# Definindo a função avalia_modelo com dois parâmetros: num_samples e log\n",
        "def avalia_modelo(num_samples = 1000, log = False):\n",
        "\n",
        "    # Iniciando um contador para as previsões corretas\n",
        "    correct = 0\n",
        "\n",
        "    # Loop que itera num_samples vezes\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        # Obtendo a entrada e o alvo (resposta correta) do i-ésimo exemplo do conjunto de teste\n",
        "        entrada, target = dataset_teste[i]\n",
        "\n",
        "        # Convertendo os tensores de entrada e alvo em arrays numpy para processamento posterior\n",
        "        entrada = entrada.cpu().numpy()\n",
        "        target = target.cpu().numpy()\n",
        "\n",
        "        # Decodificando a entrada e o alvo utilizando o tokenizer\n",
        "        entrada = tokenizer.decode(entrada[:sequence_length])\n",
        "        target = tokenizer.decode(target[sequence_length-1:])\n",
        "\n",
        "        # Gerando a previsão utilizando a função faz_previsao\n",
        "        predicted = faz_previsao(entrada, solution_length = result_length, model = modelo)\n",
        "\n",
        "        # Se a previsão for igual ao alvo, incrementa o contador de previsões corretas\n",
        "        if target == predicted:\n",
        "            correct += 1\n",
        "            # Se log for True, imprime detalhes do exemplo e a previsão correta\n",
        "            if log:\n",
        "                print(f'Acerto do Modelo: Input: {entrada} Target: {target} Previsão: {predicted}')\n",
        "        else:\n",
        "            # Se log for True, imprime detalhes do exemplo e a previsão errada\n",
        "            if log:\n",
        "                print(f'Erro do Modelo: Input: {entrada} Target: {target} Previsão: {predicted}')\n",
        "\n",
        "    # Ao final do loop, calcula a acurácia (número de previsões corretas dividido pelo número total de exemplos)\n",
        "    print(f'Acurácia: {correct/num_samples}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIMNLPmEZqpq",
        "outputId": "d2f0bd38-74ed-4bc1-e66c-e0701e452d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acerto do Modelo: Input: 2 + 4 = Target: 0 6 Previsão: 0 6\n",
            "Acerto do Modelo: Input: 6 + 0 = Target: 0 6 Previsão: 0 6\n",
            "Acerto do Modelo: Input: 8 + 1 = Target: 0 9 Previsão: 0 9\n",
            "Acerto do Modelo: Input: 3 + 0 = Target: 0 3 Previsão: 0 3\n",
            "Acerto do Modelo: Input: 4 + 5 = Target: 0 9 Previsão: 0 9\n",
            "Erro do Modelo: Input: 6 + 2 = Target: 0 8 Previsão: 0 9\n",
            "Erro do Modelo: Input: 1 + 0 = Target: 0 1 Previsão: 0 2\n",
            "Acerto do Modelo: Input: 8 + 1 = Target: 0 9 Previsão: 0 9\n",
            "Erro do Modelo: Input: 7 + 0 = Target: 0 7 Previsão: 0 8\n",
            "Erro do Modelo: Input: 6 + 6 = Target: 1 2 Previsão: 1 4\n",
            "Acurácia: 0.6\n"
          ]
        }
      ],
      "source": [
        "# Executa a função\n",
        "avalia_modelo(num_samples = 10, log = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFXzLO6BZqpq",
        "outputId": "e08751ec-d7d6-4497-b903-0ea7cecb158c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.623\n"
          ]
        }
      ],
      "source": [
        "# Executa a função\n",
        "avalia_modelo(num_samples = 1000, log = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "fim__SpSZqpr",
        "outputId": "4e13f74c-4bf0-413b-f45c-31f1eebe4b24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel</b><br/>def _call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py</a>The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input\n",
              "embeddings).\n",
              "\n",
              "\n",
              "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
              "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
              "etc.)\n",
              "\n",
              "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
              "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
              "and behavior.\n",
              "\n",
              "Parameters:\n",
              "    config ([`GPT2Config`]): Model configuration class with all the parameters of the model.\n",
              "        Initializing with a config file does not load the weights associated with the model, only the\n",
              "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 949);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "type(modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DUvGWNVhZqpr"
      },
      "outputs": [],
      "source": [
        "modelo.save_pretrained(\"modelos/modelo_final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_xZQgw9Zqpr"
      },
      "source": [
        "# Fim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCF7prfjj_up",
        "outputId": "57da04a6-e827-4717-e70a-eb435b235caf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "efXDyMFlkEDs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext watermark\n",
        "%watermark -a 'Camila' --iversions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYbvwzE_kMm3",
        "outputId": "84d137c1-5790-4b23-84eb-594d01490126"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Camila\n",
            "\n",
            "gradio      : 4.19.2\n",
            "numpy       : 1.25.2\n",
            "transformers: 4.31.0\n",
            "accelerate  : 0.27.2\n",
            "torch       : 2.0.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_llm = AutoModelForCausalLM.from_pretrained('modelos/modelo_final')"
      ],
      "metadata": {
        "id": "RehUklotkVS8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo uma classe chamada NumberTokenizer, que é usada para tokenizar os números\n",
        "class DSATokenizer:\n",
        "\n",
        "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
        "    def __init__(self, numbers_qty = 10):\n",
        "\n",
        "        # Lista de tokens possíveis que o tokenizador pode encontrar\n",
        "        vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "        # Definindo a quantidade de números que o tokenizador pode lidar\n",
        "        self.numbers_qty = numbers_qty\n",
        "\n",
        "        # Definindo o token de preenchimento (padding)\n",
        "        self.pad_token = '-1'\n",
        "\n",
        "        # Criando um dicionário que mapeia cada token para um índice único\n",
        "        self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
        "\n",
        "        # Criando um dicionário que mapeia cada índice único de volta ao token correspondente\n",
        "        self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
        "\n",
        "        # Obtendo o índice do token de preenchimento no encoder\n",
        "        self.pad_token_id = self.encoder[self.pad_token]\n",
        "\n",
        "    # Método para decodificar uma lista de IDs de token de volta para uma string\n",
        "    def decode(self, token_ids):\n",
        "        return ' '.join(self.decoder[t] for t in token_ids)\n",
        "\n",
        "    # Método que é chamado quando o objeto da classe é invocado como uma função\n",
        "    def __call__(self, text):\n",
        "        # Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\n",
        "        return [self.encoder[t] for t in text.split()]"
      ],
      "metadata": {
        "id": "9wltdkuZkfNI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria o objeto\n",
        "tokenizer = DSATokenizer(13)"
      ],
      "metadata": {
        "id": "MhaDOad8kivo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a função gera_solution com três parâmetros: input, solution_length e model\n",
        "def faz_previsao(entrada, solution_length = 6, model = modelo_llm):\n",
        "\n",
        "    # Colocando o modelo em modo de avaliação.\n",
        "    model.eval()\n",
        "\n",
        "    # Convertendo a entrada (string) em tensor utilizando o tokenizer.\n",
        "    # O tensor é uma estrutura de dados que o modelo de aprendizado de máquina pode processar.\n",
        "    entrada = torch.tensor(tokenizer(entrada))\n",
        "\n",
        "    # Iniciando uma lista vazia para armazenar a solução\n",
        "    solution = []\n",
        "\n",
        "    # Loop que gera a solução de comprimento solution_length\n",
        "    for i in range(solution_length):\n",
        "\n",
        "        # Alimentando a entrada atual ao modelo e obtendo a saída\n",
        "        saida = model(entrada)\n",
        "\n",
        "        # Pegando o índice do maior valor no último conjunto de logits (log-odds) da saída,\n",
        "        # que é a previsão do modelo para o próximo token\n",
        "        predicted = saida.logits[-1].argmax()\n",
        "\n",
        "        # Concatenando a previsão atual com a entrada atual.\n",
        "        # Isso servirá como a nova entrada para a próxima iteração.\n",
        "        entrada = torch.cat((entrada, predicted.unsqueeze(0)), dim = 0)\n",
        "\n",
        "        # Adicionando a previsão atual à lista de soluções e convertendo o tensor em um número Python padrão\n",
        "        solution.append(predicted.cpu().item())\n",
        "\n",
        "    # Decodificando a lista de soluções para obter a string de saída e retornando-a\n",
        "    return tokenizer.decode(solution)"
      ],
      "metadata": {
        "id": "au8qYOAukl4A"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa a função\n",
        "faz_previsao('3 + 5 =', solution_length = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "81ibr5AyksZa",
        "outputId": "1d181a90-e4e6-4e68-fa3a-af61ff64e9f0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0 8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para retornar a função que faz a previsão\n",
        "def funcsolve(entrada):\n",
        "    return faz_previsao(entrada, solution_length = 2)"
      ],
      "metadata": {
        "id": "KQn0cb02kqNa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria a web app\n",
        "webapp = gr.Interface(fn = funcsolve,\n",
        "                      inputs = [gr.Textbox(label = \"Dados de Entrada\",\n",
        "                                           lines = 1,\n",
        "                                           info = \"Os dados devem estar na forma: '1 + 2 =' com um único espaço entre cada caractere e apenas números de um dígito são permitidos.\")],\n",
        "                      outputs = [gr.Textbox(label = \"Resultado (Previsão do Modelo)\", lines = 1)],\n",
        "                      title = \"Deploy de LLM Após o Fine-Tuning\",\n",
        "                      description = \"Digite os dados de entrada e clique no botão Submit para o modelo fazer a previsão.\",\n",
        "                      examples = [\"5 + 3 =\", \"2 + 9 =\"])"
      ],
      "metadata": {
        "id": "AmQ3kkAMkw9e"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webapp.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "fxKi67k0lOHp",
        "outputId": "5446792f-b177-4806-8e21-a9a0e31b70b0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e909710a41f0851e17.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e909710a41f0851e17.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIM"
      ],
      "metadata": {
        "id": "-b1m3Xsrlj9Z"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f0f32650ba44908802859efc387f7d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bbed4a54c8348cf973bb77aade30405",
              "IPY_MODEL_e5141ea4c4184a76b9d08c7100e96896",
              "IPY_MODEL_dff7cc11b12c4cf9bdb27a40efa932ac"
            ],
            "layout": "IPY_MODEL_f0ffd0c2cfdf465c905209babf2110fa"
          }
        },
        "8bbed4a54c8348cf973bb77aade30405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef77cfdab1d34873894bd06fccf7ae9c",
            "placeholder": "​",
            "style": "IPY_MODEL_b9864979ec2f41c88c46becf3055a910",
            "value": "config.json: 100%"
          }
        },
        "e5141ea4c4184a76b9d08c7100e96896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_458ce2d4765c47bdb93a7611acecd673",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f624370d7a34bb5b2ac456438daebcb",
            "value": 665
          }
        },
        "dff7cc11b12c4cf9bdb27a40efa932ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbb34f81b9f148978a22d8359e17f8fd",
            "placeholder": "​",
            "style": "IPY_MODEL_9abe9c5798434432949d45497beb3995",
            "value": " 665/665 [00:00&lt;00:00, 43.8kB/s]"
          }
        },
        "f0ffd0c2cfdf465c905209babf2110fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef77cfdab1d34873894bd06fccf7ae9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9864979ec2f41c88c46becf3055a910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "458ce2d4765c47bdb93a7611acecd673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f624370d7a34bb5b2ac456438daebcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbb34f81b9f148978a22d8359e17f8fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9abe9c5798434432949d45497beb3995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}